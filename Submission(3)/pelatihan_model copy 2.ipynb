{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f42ef9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "import emoji\n",
    "import re\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26ddeff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stemmer and lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Use Gensim stopwords for English\n",
    "stop_words_gensim = STOPWORDS\n",
    "\n",
    "# Slang word normalization dictionary\n",
    "slangwords = {\n",
    "    \"@\": \"at\", \"lol\": \"laughing out loud\", \"omg\": \"oh my god\", \"idk\": \"i don't know\", \n",
    "    \"btw\": \"by the way\", \"tbh\": \"to be honest\", \"smh\": \"shaking my head\", \"brb\": \"be right back\",\n",
    "    \"bff\": \"best friends forever\", \"gtg\": \"got to go\", \"fyi\": \"for your information\", \"np\": \"no problem\",\n",
    "    \"wtf\": \"what the heck\", \"yolo\": \"you only live once\", \"fomo\": \"fear of missing out\", \"lmao\": \"laughing my ass off\",\n",
    "    \"tmi\": \"too much information\", \"srsly\": \"seriously\", \"wut\": \"what\", \"bbl\": \"be back later\", \"thx\": \"thanks\",\n",
    "    \"gr8\": \"great\", \"nvm\": \"never mind\", \"cu\": \"see you\", \"g2g\": \"got to go\", \"y\": \"why\", \"lmk\": \"let me know\",\n",
    "    \"wyd\": \"what are you doing\", \"gimme\": \"give me\", \"gonna\": \"going to\", \"wanna\": \"want to\", \"gotta\": \"got to\",\n",
    "    \"kinda\": \"kind of\", \"asap\": \"as soon as possible\", \"bday\": \"birthday\", \"bby\": \"baby\", \"cuz\": \"because\", \n",
    "    \"dr\": \"doctor\", \"b4\": \"before\", \"u\": \"you\", \"ur\": \"your\", \"pls\": \"please\", \"ty\": \"thank you\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7533cb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing functions\n",
    "def cleaningText(text):\n",
    "    \"\"\"Remove emojis, mentions, hashtags, URLs, numbers, and non-alphanumeric characters\"\"\"\n",
    "    text = str(text)\n",
    "    text = emoji.replace_emoji(text, replace='')  # Remove emojis\n",
    "    text = re.sub(r'@[A-Za-z0-9_]+', '', text)     # Remove mentions\n",
    "    text = re.sub(r'#[A-Za-z0-9_]+', '', text)     # Remove hashtags\n",
    "    text = re.sub(r'RT[\\s]+', '', text)            # Remove RT\n",
    "    text = re.sub(r\"http\\S+\", '', text)            # Remove URLs\n",
    "    text = re.sub(r'[0-9]+', '', text)             # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)            # Remove non-alphanumeric characters\n",
    "    text = text.replace('\\n', ' ')\n",
    "    return text.strip()\n",
    "\n",
    "def casefoldingText(text):\n",
    "    \"\"\"Convert to lowercase\"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "def tokenizingText(text):\n",
    "    \"\"\"Tokenize text\"\"\"\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def filteringText(tokens):\n",
    "    \"\"\"Remove stopwords\"\"\"\n",
    "    return [word for word in tokens if word not in stop_words_gensim]\n",
    "\n",
    "def stemmingText(tokens):\n",
    "    \"\"\"Apply stemming\"\"\"\n",
    "    return [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "def lemmatizingText(tokens):\n",
    "    \"\"\"Apply lemmatization\"\"\"\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "def fix_slangwords(text):\n",
    "    \"\"\"Normalize slang words\"\"\"\n",
    "    words = text.split()\n",
    "    fixed_words = [slangwords[word.lower()] if word.lower() in slangwords else word for word in words]\n",
    "    return ' '.join(fixed_words)\n",
    "\n",
    "def toSentence(tokens):\n",
    "    \"\"\"Join tokens back to sentence\"\"\"\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"Full preprocessing pipeline\"\"\"\n",
    "    text = cleaningText(text)          # 1. Cleaning\n",
    "    text = casefoldingText(text)       # 2. Case Folding\n",
    "    text = fix_slangwords(text)        # 3. Slang Normalization\n",
    "    tokens = tokenizingText(text)      # 4. Tokenizing\n",
    "    tokens = filteringText(tokens)     # 5. Stopword removal\n",
    "    tokens = stemmingText(tokens)      # 6. Stemming\n",
    "    tokens = lemmatizingText(tokens)   # 7. Lemmatization\n",
    "    return toSentence(tokens)          # 8. Join back into a sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83fbe26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi SVM: 99.25%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     negatif       0.95      0.89      0.92       134\n",
      "      netral       0.99      1.00      1.00      4010\n",
      "     positif       1.00      0.98      0.99      1856\n",
      "\n",
      "    accuracy                           0.99      6000\n",
      "   macro avg       0.98      0.96      0.97      6000\n",
      "weighted avg       0.99      0.99      0.99      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('output_sentimen.csv')\n",
    "df['clean_ulasan'] = df['ulasan'].apply(preprocess)  # Apply preprocessing\n",
    "\n",
    "# Labeling based on text (manual example)\n",
    "def label_sentiment_based_on_text(text):\n",
    "    \"\"\"Label sentiment based on text content\"\"\"\n",
    "    if \"good\" in text or \"great\" in text or \"love\" in text:\n",
    "        return 'positif'\n",
    "    elif \"bad\" in text or \"poor\" in text or \"hate\" in text:\n",
    "        return 'negatif'\n",
    "    else:\n",
    "        return 'netral'\n",
    "\n",
    "df['sentimen'] = df['clean_ulasan'].apply(label_sentiment_based_on_text)\n",
    "\n",
    "# Encode the labels (positif, negatif, netral) to numeric\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['sentimen'])\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=15000)\n",
    "X = tfidf.fit_transform(df['clean_ulasan'])\n",
    "\n",
    "# Split data (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# SVM Model\n",
    "model = SVC(kernel='linear')  # SVM dengan kernel linear\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prediction\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Akurasi SVM: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, predictions, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69beb548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Skema 1: SVM + TF-IDF + 80/20\n",
      "Akurasi: 99.25%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     negatif       0.95      0.89      0.92       134\n",
      "      netral       0.99      1.00      1.00      4010\n",
      "     positif       1.00      0.98      0.99      1856\n",
      "\n",
      "    accuracy                           0.99      6000\n",
      "   macro avg       0.98      0.96      0.97      6000\n",
      "weighted avg       0.99      0.99      0.99      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Helper untuk Word2Vec\n",
    "def get_w2v_embeddings(texts, model, vector_size):\n",
    "    embeddings = []\n",
    "    for tokens in texts:\n",
    "        vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "        if vectors:\n",
    "            embeddings.append(np.mean(vectors, axis=0))\n",
    "        else:\n",
    "            embeddings.append(np.zeros(vector_size))\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Token ulang untuk Word2Vec\n",
    "df['tokens'] = df['clean_ulasan'].apply(tokenizingText)\n",
    "\n",
    "# Train Word2Vec\n",
    "w2v_model = Word2Vec(sentences=df['tokens'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# ====================== SKEMA 1 ======================\n",
    "# SVM + TF-IDF + 80/20\n",
    "X_tfidf = tfidf.fit_transform(df['clean_ulasan'])\n",
    "y_encoded = label_encoder.fit_transform(df['sentimen'])\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X_tfidf, y_encoded, test_size=0.2, random_state=42)\n",
    "model_svm = SVC(kernel='linear')\n",
    "model_svm.fit(X_train1, y_train1)\n",
    "y_pred1 = model_svm.predict(X_test1)\n",
    "print(\"\\nðŸ”¹ Skema 1: SVM + TF-IDF + 80/20\")\n",
    "print(f\"Akurasi: {accuracy_score(y_test1, y_pred1) * 100:.2f}%\")\n",
    "print(classification_report(y_test1, y_pred1, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7cb27a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Skema 2: MLP + TF-IDF + 80/20\n",
      "Akurasi: 98.23%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     negatif       0.97      0.78      0.87       134\n",
      "      netral       0.98      1.00      0.99      4010\n",
      "     positif       0.99      0.96      0.98      1856\n",
      "\n",
      "    accuracy                           0.98      6000\n",
      "   macro avg       0.98      0.91      0.94      6000\n",
      "weighted avg       0.98      0.98      0.98      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ====================== SKEMA 2 ======================\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=15000)\n",
    "X_tfidf_mlp = tfidf_vectorizer.fit_transform(df['clean_ulasan'])\n",
    "X_train5, X_test5, y_train5, y_test5 = train_test_split(X_tfidf_mlp, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "model_mlp = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=300, random_state=42)\n",
    "model_mlp.fit(X_train5, y_train5)\n",
    "y_pred5 = model_mlp.predict(X_test5)\n",
    "\n",
    "print(\"\\nðŸ”¹ Skema 2: MLP + TF-IDF + 80/20\")\n",
    "print(f\"Akurasi: {accuracy_score(y_test5, y_pred5) * 100:.2f}%\")\n",
    "print(classification_report(y_test5, y_pred5, target_names=label_encoder.classes_))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e14acc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Skema 3: RF + TF-IDF + 70/30\n",
      "Akurasi: 99.03%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     negatif       1.00      0.75      0.86       206\n",
      "      netral       0.99      1.00      0.99      6003\n",
      "     positif       1.00      0.99      0.99      2791\n",
      "\n",
      "    accuracy                           0.99      9000\n",
      "   macro avg       0.99      0.91      0.95      9000\n",
      "weighted avg       0.99      0.99      0.99      9000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ====================== SKEMA 3 ======================\n",
    "# RF + TF-IDF + 70/30\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X_tfidf, y_encoded, test_size=0.3, random_state=42)\n",
    "model_rf_tfidf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_rf_tfidf.fit(X_train3, y_train3)\n",
    "y_pred3 = model_rf_tfidf.predict(X_test3)\n",
    "print(\"\\nðŸ”¹ Skema 3: RF + TF-IDF + 70/30\")\n",
    "print(f\"Akurasi: {accuracy_score(y_test3, y_pred3) * 100:.2f}%\")\n",
    "print(classification_report(y_test3, y_pred3, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f025aeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
